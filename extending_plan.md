# Extending Plan — Middle Ground: Plugins + Monitoring

## Goal

Create a maintainable middle-ground architecture that: (1) keeps `yt-dlp` as the default backend, (2) allows custom, site-specific extractors to be added as plugins per-domain, and (3) provides a monitoring/test harness to detect extractor regressions early.

## Constraints

- Do not modify `downloader.py` as part of this plan. The plan describes external modules, tests, and changes you can implement.
- Keep changes minimal and incremental; prioritize a working prototype for one site, then generalize.

## Deliverables (concrete)

- `ext/plugins/README.md` (how to write a plugin)
- `ext/plugins/example_site.py` (prototype extractor for one site)
- `ext/registry.py` (lightweight plugin registry + loader)
- `tests/test_plugin_api.py` (unit tests for the plugin interface)
- `tools/monitor.py` (small monitoring harness to run checks)
- CI workflow to run monitoring periodically (optional, described here)

## Design overview

1. Plugin extractor modules implement a small, well-documented interface (see "Plugin API" below).
2. A runtime registry maps domains (or matchers) to extractor modules. When a URL is requested, the registry is consulted first; if no registered extractor handles the URL, fall back to `yt-dlp`.
3. The downloader's core (you will not change `downloader.py` now) can be left untouched; instead, the registry can expose an API that your wrapper calls before calling `yt-dlp`.

**Part A — Prototype a custom extractor for one target site**
------------------------------------------------------------

Objective: write a single-site extractor that can: identify the media, return metadata and direct media URLs (or HLS m3u8), and allow the wrapper to download or hand the URL to `ffmpeg`.

1) Choose target site and goals
   - Pick a single site you want to support (e.g., `miruro.to` from your earlier example).
   - Decide whether you need to support login, region checks, or paywalls.

2) Environment & Dependencies
   - Use Python 3.11+ (project already uses 3.13 in tests). Add these extras in `requirements-dev.txt` if needed:
     - `requests` for static pages
     - `beautifulsoup4` or `lxml` for HTML parsing
     - `playwright` or `selenium` if JS rendering is required (prefer `playwright`)
     - `m3u8` for parsing HLS playlists (optional)

3) Module layout
   - Create `ext/plugins/example_site.py` with the extractor class and helper functions
   - Add `ext/plugins/__init__.py` exporting a small registry hook example

4) Extractor interface (minimal)
   - Required functions and return types:

     def can_handle(url: str) -> bool
         """Return True if this extractor is a good candidate for this URL."""

     def extract(url: str, *, session: requests.Session | None = None, headers: dict | None = None, cookies: str | None = None, extractor_args: dict | None = None) -> dict
         """Return info dict describing available media.

         Required keys in returned dict:
         - `id`: string id
         - `title`: human-readable title
         - `formats`: list of dicts, each with keys `url`, `ext`, `format_id`, `protocol` (e.g., "http","hls"), `filesize` (optional)
         - `subtitles`: dict (optional)
         - `is_live`: bool
         """

   - Extraction should NOT perform heavy downloading. It returns URLs and metadata.

5) Implementation approach
   - Try static HTTP flow first: fetch page HTML with `requests`, parse for `<video>` tags, `<source>` urls, JSON blobs (`<script>` tags) or data-* attributes.
   - If data is loaded via XHR, mimic the XHR API (use Developer Tools Network tab to discover API endpoints and the required request headers/cookies).
   - If content is generated by complex JS, use `playwright` headless to evaluate and extract the final player payload (call `page.content()` after waiting for network idle) or run `page.evaluate()` to read JavaScript variables.
   - For HLS (m3u8): return the m3u8 URL in `formats` with `protocol: hls`. The downloader can use `ffmpeg` to download/convert.

6) Local test plan
   - Unit test `can_handle` using representative URLs.
   - Unit test `extract` against a saved HTML fixture (benchmark sample). Use `responses` or `requests-mock` to return fixture content for reproducible tests.
   - Integration test: call `extract()` against live site (optional, marked as `integration` and skipped in CI by default).

7) Example minimal extractor snippet
   (put this in `ext/plugins/example_site.py`)

```py
import re
import requests
from bs4 import BeautifulSoup

SITE_RE = re.compile(r"https?://(www\.)?miruro\.to/")

def can_handle(url: str) -> bool:
    return bool(SITE_RE.match(url))

def extract(url: str, *, session: requests.Session | None = None, headers=None, cookies=None, extractor_args=None):
    s = session or requests.Session()
    r = s.get(url, headers=headers)
    r.raise_for_status()
    html = r.text
    soup = BeautifulSoup(html, "lxml")
    # find <video> or JSON blobs
    video_tag = soup.find("video")
    formats = []
    if video_tag:
        for src in video_tag.find_all("source"):
            formats.append({"url": src["src"], "ext": src.get("type", "mp4"), "protocol": "http", "format_id": "http"})
    # fallback: search scripts for playlist URL
    # TODO: parse additional structures
    info = {"id": url, "title": soup.title.string if soup.title else url, "formats": formats, "is_live": False}
    return info
```

**Part B — Add a plugin API so custom extractors can be registered per-domain**
----------------------------------------------------------------------------

Objective: design a simple registry so the downloader can consult plugins first.

1) Registry responsibilities
   - Discover plugin modules (search `ext/plugins` directory, or load via setuptools entry points `downloader.plugins`).
   - Register mapping from domain/matcher -> module
   - Provide `find_extractor(url)` that returns a module implementing `can_handle` and `extract`.

2) API sketch (file: `ext/registry.py`)

```py
from importlib import import_module
from pathlib import Path
from typing import Optional

PLUGIN_PATH = Path(__file__).parent / "plugins"

def discover_plugins():
    # simple: import all .py in ext/plugins
    plugins = []
    for p in PLUGIN_PATH.glob("*.py"):
        name = p.stem
        if name == "__init__":
            continue
        mod = import_module(f"ext.plugins.{name}")
        plugins.append(mod)
    return plugins

_PLUGINS = discover_plugins()

def find_extractor(url: str):
    for mod in _PLUGINS:
        try:
            if getattr(mod, "can_handle", lambda u: False)(url):
                return mod
        except Exception:
            continue
    return None
```

1) Integration strategy (without changing `downloader.py` yet)
   - Add a small shim script `tools/wrapper.py` that first calls `find_extractor(url)`. If found, call `extract()` and either hand results to `ffmpeg` for download or to `yt-dlp` (if it can accept a direct URL). If not found, call the existing `downloader.py` (or its internals) as fallback.

2) Plugin packaging
   - Encourage plugin authors to implement pure-Python modules with `can_handle` and `extract` functions.
   - Optionally support setuptools entry points `downloader.plugins` for discoverability if you later package plugins separately.

3) Security and isolation
   - Plugins are arbitrary code. If you plan to run third-party plugins, consider sandboxing (not trivial in Python). For now, recommend audited/first-party plugins.

**Part C — Keep `yt-dlp` as default and implement monitoring/test harness for failing sites**
-------------------------------------------------------------------------------------------

Objective: Detect when extractors (custom or yt-dlp) stop working; run checks frequently and fail fast in CI.

1) Monitoring harness (`tools/monitor.py`)
   - Maintain a JSON or YAML config file `tools/monitor_targets.yml` with a list of target URLs, expected keys (title regex, min formats), and recommended check cadence.
   - For each target, test both plugin extractor (if exists) and `yt-dlp` extraction, and record results.
   - If extractor fails while `yt-dlp` succeeds (or vice versa), record and notify.

2) Implementation details
   - For `yt-dlp` checks, call `yt_dlp.YoutubeDL(ydl_opts).extract_info(url, download=False)` and capture exceptions and returned info.
   - For plugin checks, import registry and call `extract()`; validate returned dict against schema.
   - Persist results to a JSON log file (timestamped) and compute diffs between runs.

3) Alerts & CI
   - Add a GitHub Actions workflow `monitor.yml` that runs `tools/monitor.py` on a schedule (e.g., daily) or on PRs. If failures are detected, create an issue or send an email/webhook.
   - For fast feedback, add a `smoke` job that runs a small set of public target URLs on every PR.

4) Tests for monitoring
   - Unit tests for `tools/monitor.py` using `responses` or `requests-mock` to simulate site responses and `yt-dlp` calls mocked to return simple info.

Testing strategy (local & CI)
----------------------------

- Unit tests: small fixtures, local HTML files, `requests-mock` responses, test extractor parsing functions.
- Integration tests: live-check tests that are marked `integration` and skipped on default CI runs; optionally run nightly.
- Regression recording: store canonical outputs (JSON) from an extractor and fail tests if the new output differs significantly.

Example test files to add
-------------------------

- `tests/test_plugin_api.py` — verify `can_handle` and `extract` contract using fixtures.
- `tests/test_monitor.py` — test monitoring logic with mocked `yt-dlp` and plugin modules.

Operational guidance
--------------------

- Prioritize sites your users care about; don’t attempt to reimplement a general-purpose scraping engine.
- Prefer `requests` + parsing for simple sites; use `playwright` only when necessary (JS-heavy pages).
- Use fixtures and `requests-mock` to make unit tests deterministic and fast.

Legal & ethical notes
---------------------

- Confirm that scraping/downloading content from a target site is permitted by its TOS and local law. Respect robots.txt where appropriate and avoid bypassing paywalls or DRM.

Acceptance criteria (for each phase)
-----------------------------------

- Prototype extractor: `can_handle(url)` correctly matches target URLs; `extract(url)` returns an info dict for test fixtures.
- Plugin API: registry can discover and return plugin modules; `find_extractor(url)` returns the right module for test URLs.
- Monitoring: `tools/monitor.py` runs and reports pass/fail for a small set of targets; CI can run a smoke test and fail when extraction fails.

Prioritized next steps (what to do first)
----------------------------------------

1. Implement `ext/plugins/example_site.py` and `tests/test_plugin_api.py` using a saved HTML fixture. (High priority — proves interface.)
2. Implement `ext/registry.py` with a simple import-based discovery. Add unit tests.
3. Create `tools/wrapper.py` shim that demonstrates using the registry and falling back to `yt-dlp`.
4. Implement `tools/monitor.py` with a small `monitor_targets.yml` and run locally.
5. Add CI jobs: a `smoke` test on PRs and scheduled monitoring runs nightly.

## Appendix — Suggested JSON schema for extractor info

```json
{
  "id": "string",
  "title": "string",
  "description": "string (optional)",
  "formats": [
    {"format_id": "string", "url": "string", "ext": "mp4", "protocol": "http|hls|dash", "filesize": 12345}
  ],
  "thumbnails": [{"url":"...","width":...}],
  "subtitles": {"en": [{"url":"..."}]},
  "is_live": false
}
```
